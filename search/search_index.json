{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome Page","text":"<p>Welcome to Computer Vision Theory! On this website you can find a the complete theory and how tos of computer vision!</p>"},{"location":"introduction/","title":"Introduction","text":""},{"location":"introduction/#what-is-computer-vision","title":"What is computer vision?","text":"<p>Computer vision is the automatic extraction of \"meaningful\" information from images and videos.</p> <p> </p>"},{"location":"introduction/#why-study-computer-vision","title":"Why study computer vision?","text":"<ul> <li>Relieve humans of boring, easy tasks</li> <li>Enhance human abilities: human-computer interaction, visualization, augmented reality (AR)</li> <li>Perception for autonomous robots</li> <li>Organize and give access to visual content</li> </ul>"},{"location":"introduction/#vision-in-humans","title":"Vision in Humans","text":"<ul> <li>Vision is our most powerful sense. Half of primate cerebral cortex is devoted to visual processing.</li> <li>The retina is ~1,000 mm2. Contains 130 million photoreceptors (120 mil. rods for low light vision and 10 mil. cones for color sampling) covering a visual field of 220x135 degrees.</li> <li>Provides enormous amount of information: data-rate of ~3 GBytes/s.</li> <li>To match the eye resolution, we would need a 500 Megapixel camera. But in practice the acuity of an eye is 8 Megapixels within a 18-degree field of view (5.5 mm diameter) around a region called fovea.</li> </ul>"},{"location":"introduction/#how-we-see","title":"How We See","text":"<ul> <li>The area we see in focus and in full color represents the part of the visual field that is covered by the fovea.</li> <li>The fovea is 0.35 mm in diameter, covers a visual field of 1-2 degrees, has high density of cone cells.</li> <li>Within the rest of the peripheral visual field, the image we perceive becomes more blurry (rod cells)</li> </ul>"},{"location":"introduction/#origins-of-computer-vision","title":"Origins of Computer Vision","text":"<ul> <li>1963 - L. G. Roberts publishes his PhD thesis on Machine Perception of Three Dimensional Solids, thesis, MIT Department of Electrical Engineering<ul> <li>He is the inventor of ARPANET, the current Internet</li> </ul> </li> <li>1966 \u2013 Seymour Papert, MIT, publishes the Summer Vision Project asking students to design an algorithm to segment an image into objects and background\u2026 within one summer!</li> <li>1969 \u2013 David Marr starts developing a framework for processing visual information</li> </ul>"},{"location":"introduction/#computer-vision-vs-computer-graphics","title":"Computer Vision Vs. Computer Graphics","text":"<p>The two problems are inverted: analysis vs. synthesis</p> <p> </p>"},{"location":"introduction/#reference-textbooks","title":"Reference Textbooks","text":"<ul> <li>Computer Vision: Algorithms and Applications, by Richard Szeliski, 2009. PDF freely downloadable from the author webpage: http://szeliski.org/Book/</li> <li>Chapter 4 of Autonomous Mobile Robots, by R. Siegwart, I.R. Nourbakhsh, D. Scaramuzza PDF</li> <li>Alternative books:<ul> <li>Robotics, Vision and Control: Fundamental Algorithms, by Peter Corke 2011.</li> <li>An Invitation to 3D Vision: Y. Ma, S. Soatto, J. Kosecka, S.S. Sastry</li> <li>Multiple view Geometry: R. Hartley and A. Zisserman</li> </ul> </li> </ul>"},{"location":"Structure_from_Motion/1_sfm/","title":"Structure from Motion (SfM)","text":""},{"location":"Structure_from_Motion/1_sfm/#problem-formulation","title":"Problem Formulation","text":"<p>Given a set of \\(n\\) point correspondences between two images, \\(p_1^i = (u_1^i, v_1^i)\\), \\(p_2^i = (u_2^i, v_2^i)\\), where \\(i=1 \\dots n\\), the goal is to simultaneously:</p> <ul> <li>Estimate the 3D points \\(P^i\\)</li> <li>The camera relative-motion parameters \\((R, T)\\)</li> <li> <p>The camera intrinsics \\(K_1\\), \\(K_2\\) that satisfy:</p> \\[ \\begin{cases}  \\lambda_1 \\begin{bmatrix} u_1^i \\\\ v_1^i \\\\ 1 \\end{bmatrix} = K_1 \\begin{bmatrix} I &amp; 0 \\end{bmatrix} \\begin{bmatrix} X_w^i \\\\ Y_w^i \\\\ Z_w^i \\\\ 1 \\end{bmatrix} \\\\ \\lambda_2 \\begin{bmatrix} u_2^i \\\\ v_2^i \\\\ 1 \\end{bmatrix} = K_2 \\begin{bmatrix} R &amp; T \\end{bmatrix} \\begin{bmatrix} X_w^i \\\\ Y_w^i \\\\ Z_w^i \\\\ 1 \\end{bmatrix} \\end{cases} \\] <p> </p> </li> </ul>"},{"location":"Structure_from_Motion/1_sfm/#two-problem-variants","title":"Two Problem Variants","text":"<p>Two variants of this problem exist:</p> <ul> <li>Calibrated camera(s) \\(\\rightarrow K_1\\), \\(K_2\\) are known</li> <li>Uncalibrated camera(s) \\(\\rightarrow K_1\\), \\(K_2\\) are unknown</li> </ul>"},{"location":"Structure_from_Motion/2_calib_cams/","title":"SfM with Calibrated Cameras","text":"<p>In the case where the cameras used are calibrated, we can use the known \\(K\\) matrices to our advantage. For convenience, let's use normalized image coordinates:</p> \\[ \\begin{bmatrix}     \\bar{u} \\\\     \\bar{v} \\\\     1 \\end{bmatrix} = K^{-1} \\begin{bmatrix}     u \\\\     v \\\\     1 \\end{bmatrix} \\] <p>With \\(K\\) known, we want to find \\(R\\), \\(T\\) and \\(P^i\\) that satisfy:</p> \\[ \\begin{cases}  \\lambda_1 \\begin{bmatrix} \\bar{u}_1^i \\\\ \\bar{v}_1^i \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} I &amp; 0 \\end{bmatrix} \\begin{bmatrix} X_w^i \\\\ Y_w^i \\\\ Z_w^i \\\\ 1 \\end{bmatrix} \\\\ \\lambda_2 \\begin{bmatrix} \\bar{u}_2^i \\\\ \\bar{v}_2^i \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} R &amp; T \\end{bmatrix} \\begin{bmatrix} X_w^i \\\\ Y_w^i \\\\ Z_w^i \\\\ 1 \\end{bmatrix} \\end{cases} \\]"},{"location":"Structure_from_Motion/2_calib_cams/#scale-ambiguity","title":"Scale Ambiguity","text":"<p>If we rescale the entire scene and camera views by a constant factor (i.e. similarity transformation), the projections (in pixels) of the scene points in both images remain exactly the same.</p> <p> </p> <p>In Structure from Motion, it is therefore not possible to recover the absolute scale of the scene! - We can do this with stereo vision, because we have prior knowledge of the transformation between cameras (the baseline between the values gives a parameter in physical units).</p> <p>Thus, only 5 degrees of freedom are measurable:</p> <ul> <li>3 parameters to describe the rotation</li> <li>2 parameters for the translation up to a scale (we can only compute the direction of translation but not its length)</li> </ul>"},{"location":"Structure_from_Motion/2_calib_cams/#knowns-unknowns","title":"Knowns &amp; Unknowns","text":"<p>How many knowns and unknowns?</p> <ul> <li>\\(4n\\) knowns:<ul> <li>\\(n\\) correspondences, each one \\((u_1^i, v_1^i)\\) and \\((u_2^i, v_2^i)\\) for \\(i=1\\dots n\\)</li> </ul> </li> <li>\\(5+3n\\) unknowns:<ul> <li>5 for the motion up to a scale (3 for rotation, 2 for translation)</li> <li>\\(3n\\) = number of coordinates of the \\(n\\) 3D points</li> </ul> </li> </ul> <p>Does a solution exist?</p> <ul> <li> <p>If and only if the number of independent equations \u2265 number of unknowns:</p> \\[ 4n \\geq 5+3n \\quad\\rightarrow\\quad n\\geq 5 \\] </li> </ul>"},{"location":"Structure_from_Motion/2_calib_cams/#relative-motion-estimation","title":"Relative Motion Estimation","text":"<p>Can we solve the estimation of relative motion \\((R, T)\\) independently of the estimation of the 3D points? Yes! This section proves that this is possible.</p>"},{"location":"Structure_from_Motion/2_calib_cams/#the-epipolar-constraint-recap","title":"The Epipolar Constraint (Recap)","text":"<ul> <li>The camera centers \\(C_1\\), \\(C_2\\) and one image point \\(p_1\\) (or \\(p_2\\)) determine the so called epipolar plane.</li> <li>The intersections of the epipolar plane with the two image planes are called epipolar lines.</li> <li>Corresponding points must therefore lie along the epipolar lines: This constraint is called the epipolar constraint</li> <li>And alternative way to formulate the epipolar constraint is to notice that two corresponding image vectors plus the baseline must be coplanar.</li> </ul> <p>We know that \\(\\bar{p}_1\\), \\(\\bar{p}_2\\) and \\(T\\) are coplanar :</p> \\[ \\bar{p}_2^T \\cdot n = 0 \\rightarrow \\bar{p}_2^T \\cdot (T \\times \\bar{p}'_1) = 0 \\rightarrow \\bar{p}_2^T(T \\times (R\\bar{p}_1)) = 0 \\rightarrow \\bar{p}_2^T [T_\\times R] \\bar{p}_1 = 0 \\] <p>Tip</p> <p>\\(\\bar{p}'_1\\) is the vector \\(\\bar{p}_1\\) in the second camera's frame.</p> <p>The resulting equation from the deduction above is the epipolar constraint:</p> \\[ \\bar{p}_2^T E \\bar{p}_1^T = 0 \\] <p>The the matrix \\(E\\) is known as the essential matrix:</p> \\[ E = [T_\\times] R \\]"},{"location":"Structure_from_Motion/2_calib_cams/#how-to-compute-the-essential-matrix","title":"How to compute the Essential matrix?","text":"<p>If we don't know \\(R\\), \\(T\\), can we estimate \\(E\\) from two images?</p> <ul> <li>Yes, given at least 5 correspondences</li> </ul>"},{"location":"Structure_from_Motion/2_calib_cams/#the-8-point-algorithm","title":"The 8-Point Algorithm","text":"<p>Each pair of correspondences \\(\\bar{p}_1 = (\\bar{u}_1, \\bar{v}_1, 1)^T\\), \\(\\bar{p}_2 = (\\bar{u}_2, \\bar{v}_2, 1)^T\\) provides a linear equation:</p> \\[ \\bar{p}_2^T E \\bar{p}_1 = 0 \\quad\\text{with}\\quad E = \\begin{bmatrix} e_{11} &amp; e_{12} &amp; e_{13} \\\\ e_{21} &amp; e_{22} &amp; e_{23} \\\\ e_{31} &amp; e_{32} &amp; e_{33} \\end{bmatrix} \\] <p>Leading to the equation:</p> \\[ \\bar{u}_2 \\bar{u}_1 e\\_{11} + \\bar{u}_2 \\bar{v}_1 e\\_{12} + \\bar{u}_2 e\\_{13} + \\bar{v}_2 \\bar{u}_1 e\\_{21} + \\bar{v}_2 \\bar{v}_1 e\\_{22} + \\bar{v}_2 e\\_{23} + \\bar{u}_1 e\\_{31} + \\bar{v}_1 e\\_{32} + e\\_{33} = 0 \\] <p>for each point in 3D space seen by the two cameras. For \\(n\\) points, we can write \\(QE = 0\\), which expands to:</p> \\[ \\begin{bmatrix} \\bar{u}_2^1 \\bar{u}_1^1 &amp; \\bar{u}_2^1 \\bar{v}_1^1 &amp; \\bar{u}_2^1 &amp; \\bar{v}_2^1 \\bar{u}_1^1 &amp; \\bar{v}_2^1 \\bar{v}_1^1 &amp; \\bar{v}_2^1 &amp; \\bar{u}_1^1 &amp; \\bar{v}_1^1 &amp; 1 \\\\\\\\ \\bar{u}_2^2 \\bar{u}_1^2 &amp; \\bar{u}_2^2 \\bar{v}_1^2 &amp; \\bar{u}_2^2 &amp; \\bar{v}_2^2 \\bar{u}_1^2 &amp; \\bar{v}_2^2 \\bar{v}_1^2 &amp; \\bar{v}_2^2 &amp; \\bar{u}_1^2 &amp; \\bar{v}_1^2 &amp; 1 \\\\\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\\\\\ \\bar{u}_2^n \\bar{u}_1^n &amp; \\bar{u}_2^n \\bar{v}_1^n &amp; \\bar{u}_2^n &amp; \\bar{v}_2^n \\bar{u}_1^n &amp; \\bar{v}_2^n \\bar{v}_1^n &amp; \\bar{v}_2^n &amp; \\bar{u}_1^n &amp; \\bar{v}_1^n &amp; 1 \\end{bmatrix} \\begin{bmatrix} e_{11} \\\\ e_{12} \\\\ e_{13} \\\\ e_{21} \\\\ e_{22} \\\\ e_{23} \\\\ e_{31} \\\\ e_{32} \\\\ e_{33} \\end{bmatrix} = 0 \\] <p>The matrix \\(Q\\) is known, whereas the matrix \\(E\\) is unknown.</p>"},{"location":"Structure_from_Motion/2_calib_cams/#solving-the-8-point-algorithm","title":"Solving the 8-Point Algorithm","text":"<p>Minimal Solution</p> <ul> <li>\\(Q_{n\\times 9}\\) should have rank 8 to have a unique (up to scale) non-trivial solution \\(\\bar{E}\\)</li> <li>Each point correspondence provides 1 independent equation</li> <li>Thus, 8 point correspondences are needed</li> </ul> <p>Over-Determined Solution</p> <ul> <li>\\(n&gt;8\\) points</li> <li>A solution is to minimize \\(\\|Q\\bar{E}\\|^2\\) subject to the constraint \\(\\|\\bar{E}\\|^2 = 1\\)     The solution is the eigenvector corresponding to the smallest eigenvalue of the matrix \\(Q^T Q\\) (because it is the unit vector \\(x\\) that minimizes \\(\\|Qx\\|^2 = x^T Q^T Q x\\)).</li> <li>It can be solved through singular value decomposition (SVD)</li> </ul> <p>Degenerate Configurations</p> <ul> <li>The solution of the 8-point algorithm is degenerate when the 3D points are coplanar.</li> <li>Conversely, the 5-point algorithm works also for coplanar points.</li> </ul>"},{"location":"Structure_from_Motion/2_calib_cams/#extract-rotation-and-translation","title":"Extract Rotation and Translation","text":"<p>Singular value decomposition yields the following: \\(E = U \\Sigma V^T\\).</p> <p>Enforcing a rank-2 constraint: We set the smallest singular value of \\(\\Sigma\\) to 0:</p> \\[ \\Sigma = \\begin{bmatrix}     \\sigma_1 &amp; 0 &amp; 0 \\\\     0 &amp; \\sigma_2 &amp; 0 \\\\     0 &amp; 0 &amp; \\sigma_3     \\end{bmatrix} = \\begin{bmatrix}     \\sigma_1 &amp; 0 &amp; 0 \\\\     0 &amp; \\sigma_2 &amp; 0 \\\\     0 &amp; 0 &amp; 0     \\end{bmatrix} \\] \\[ \\hat{T} = U \\begin{bmatrix}     0 &amp; \\mp 1 &amp; 0 \\\\     \\pm 1 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0     \\end{bmatrix} \\Sigma V^T \\quad \\text{where} \\quad \\hat{T} = \\begin{bmatrix}     0 &amp; -t_z &amp; t_y \\\\     t_z &amp; 0 &amp; t_x \\\\     -t_y &amp; t_x &amp; 0     \\end{bmatrix} \\rightarrow \\hat{t} = \\begin{bmatrix} t_x \\\\ t_y \\\\ t_z \\end{bmatrix} \\] \\[ \\hat{R} = U \\begin{bmatrix}     0 &amp; \\mp 1 &amp; 0 \\\\     \\pm 1 &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; 0     \\end{bmatrix} V^T \\] <p>Therefore \\(R\\) and \\(T\\) can be found as follows:</p> \\[ \\begin{align}     T &amp;= K_2 \\hat{t} \\\\     R &amp;= K_2 \\hat{R} K_1^{-1} \\end{align} \\] <p>Such an answer gives four possible solutions. However, there is only one solution where the points are in front of both cameras:</p> <p> </p>"},{"location":"Structure_from_Motion/3_uncalib_cams/","title":"SfM with Uncalibrated Cameras","text":"<p>While can and is used with known intrinsic camera parameters, it's big advantage over other methods is that it can be used with uncalibrated cameras by using the fundamental matrix instead of the essential matrix.</p>"},{"location":"Structure_from_Motion/3_uncalib_cams/#the-fundamental-matrix","title":"The Fundamental Matrix","text":"<p>So far, we have assumed to know the camera intrinsic parameters and we have used normalized image coordinates to get the epipolar constraint for calibrated cameras:</p> \\[ \\begin{bmatrix} \\overline{u}^i_1 \\\\ \\overline{v}^i_1 \\\\ 1 \\end{bmatrix} = K_1^{-1} \\begin{bmatrix} u^i_1 \\\\ v^i_1 \\\\ 1 \\end{bmatrix} \\qquad \\begin{bmatrix} \\overline{u}^i_2 \\\\ \\overline{v}^i_2 \\\\ 1 \\end{bmatrix} = K_2^{-1} \\begin{bmatrix} u^i_2 \\\\ v^i_2 \\\\ 1 \\end{bmatrix} \\] \\[ \\overline{p}_2^T E \\overline{p}_1 = 0 \\] \\[ \\begin{bmatrix} \\overline{u}^i_2 \\\\ \\overline{v}^i_2 \\\\ 1 \\end{bmatrix}^T E \\begin{bmatrix} \\overline{u}^i_1 \\\\ \\overline{v}^i_1 \\\\ 1 \\end{bmatrix} = 0 \\] <p>We can modify the last equation to create the fundamental matrix \\(F\\):</p> \\[ \\begin{bmatrix} \\overline{u}^i_2 \\\\ \\overline{v}^i_2 \\\\ 1 \\end{bmatrix}^T \\underbrace{K_2^{-T} E K_1^{-1}}_{=F} \\begin{bmatrix} \\overline{u}^i_1 \\\\ \\overline{v}^i_1 \\\\ 1 \\end{bmatrix} = 0 \\] \\[ F = K_2^{-T} E K_1^{-1} \\]"},{"location":"Structure_from_Motion/3_uncalib_cams/#the-8-point-algorithm-for-the-fundamental-matrix","title":"The 8-Point Algorithm for the Fundamental Matrix","text":"<p>The same 8-point algorithm to compute the essential matrix from a set of normalized image coordinates can also be used to determine the fundamental matrix:</p> \\[ \\begin{bmatrix} \\overline{u}^i_2 \\\\ \\overline{v}^i_2 \\\\ 1 \\end{bmatrix}^T F \\begin{bmatrix} \\overline{u}^i_1 \\\\ \\overline{v}^i_1 \\\\ 1 \\end{bmatrix} = 0 \\] <p>However, now the key advantage is that we work directly in pixel coordinates.</p> <p>Info</p> <p>The 8 point algorithm works for both calibrated and uncalibrated cameras, however the 5 point algorithm only works for calibrated cameras.</p> <p>Similarly to solving the problem with the essential matrix we need to use SVD to solve the following:</p> \\[ \\begin{bmatrix} \\bar{u}_2^1 \\bar{u}_1^1 &amp; \\bar{u}_2^1 \\bar{v}_1^1 &amp; \\bar{u}_2^1 &amp; \\bar{v}_2^1 \\bar{u}_1^1 &amp; \\bar{v}_2^1 \\bar{v}_1^1 &amp; \\bar{v}_2^1 &amp; \\bar{u}_1^1 &amp; \\bar{v}_1^1 &amp; 1 \\\\\\\\ \\bar{u}_2^2 \\bar{u}_1^2 &amp; \\bar{u}_2^2 \\bar{v}_1^2 &amp; \\bar{u}_2^2 &amp; \\bar{v}_2^2 \\bar{u}_1^2 &amp; \\bar{v}_2^2 \\bar{v}_1^2 &amp; \\bar{v}_2^2 &amp; \\bar{u}_1^2 &amp; \\bar{v}_1^2 &amp; 1 \\\\\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\\\\\ \\bar{u}_2^n \\bar{u}_1^n &amp; \\bar{u}_2^n \\bar{v}_1^n &amp; \\bar{u}_2^n &amp; \\bar{v}_2^n \\bar{u}_1^n &amp; \\bar{v}_2^n \\bar{v}_1^n &amp; \\bar{v}_2^n &amp; \\bar{u}_1^n &amp; \\bar{v}_1^n &amp; 1 \\end{bmatrix} \\begin{bmatrix} f_{11} \\\\ f_{12} \\\\ f_{13} \\\\ f_{21} \\\\ f_{22} \\\\ f_{23} \\\\ f_{31} \\\\ f_{32} \\\\ f_{33} \\end{bmatrix} = 0 \\]"},{"location":"Structure_from_Motion/3_uncalib_cams/#problem-with-8-point-algorithm","title":"Problem with 8-Point Algorithm","text":"<p>When using SVD to solve the equation above, we have poor numerical conditioning, making our solution very sensitive to noise. This can fixed by rescaling the input data, as shown in the example below:</p> <p>Example</p> <p>It can be common to have the input data as shown below. The differences in magnitude between the columns is readily visible in the matrix below:</p> \\[ \\begin{bmatrix} 250906.36 &amp; 183269.57 &amp; 921.81 &amp; 200931.1 &amp; 146766.13 &amp; 738.21 &amp; 272.19 &amp; 198.81 &amp; 1.00 \\\\ 2692.28 &amp; 131633.03 &amp; 176.27 &amp; 6196.73 &amp; 302975.59 &amp; 405.71 &amp; 15.27 &amp; 746.79 &amp; 1.00 \\\\ 416374.23 &amp; 871684.3 &amp; 935.47 &amp; 408110.89 &amp; 854384.92 &amp; 916.9 &amp; 445.1 &amp; 931.81 &amp; 1.00 \\\\ 191183.6 &amp; 171759.4 &amp; 410.27 &amp; 416435.62 &amp; 374125.9 &amp; 893.65 &amp; 465.99 &amp; 418.65 &amp; 1.00 \\\\ 48988.86 &amp; 30401.76 &amp; 57.89 &amp; 298604.57 &amp; 185309.58 &amp; 352.87 &amp; 846.22 &amp; 525.15 &amp; 1.00 \\\\ 164786.04 &amp; 546559.67 &amp; 813.17 &amp; 1998.37 &amp; 6628.15 &amp; 9.86 &amp; 202.65 &amp; 672.14 &amp; 1.00 \\\\ 116407.01 &amp; 2727.75 &amp; 138.89 &amp; 169941.27 &amp; 3982.21 &amp; 202.77 &amp; 838.12 &amp; 19.64 &amp; 1.00 \\\\ 135384.58 &amp; 75411.13 &amp; 198.72 &amp; 411350.03 &amp; 229127.78 &amp; 603.79 &amp; 681.28 &amp; 379.48 &amp; 1.00 \\end{bmatrix} \\begin{bmatrix} f_{11} \\\\ f_{12} \\\\ f_{13} \\\\ f_{21} \\\\ f_{22} \\\\ f_{23} \\\\ f_{31} \\\\ f_{32} \\\\ f_{33} \\end{bmatrix} = 0 \\] <p>Such differences in magnitude means that solving by least-squares (SVD) will yield poor results.</p>"},{"location":"Structure_from_Motion/3_uncalib_cams/#normalized-8-point-algorithm","title":"Normalized 8-Point Algorithm","text":"<p>This can be fixed using a normalized 8-point algorithm [Hartley, 1997]1, which estimates the Fundamental matrix on a set of normalized correspondences (with better numerical properties) and then un-normalizes the result to obtain the fundamental matrix for the given (un-normalized) correspondences.</p> <p>Idea: Transform image coordinates so that they are in the range \\(\\sim[-1, 1] \\times [-1, 1]\\).</p>"},{"location":"Structure_from_Motion/3_uncalib_cams/#rescale-shift-method","title":"Rescale &amp; Shift Method","text":"<p>The most common method is to apply the rescale and shift method (shown below).</p> <p> </p>"},{"location":"Structure_from_Motion/3_uncalib_cams/#zero-centroid-method","title":"Zero-Centroid Method","text":"<p>In the original 1997 paper1, Hartley proposed to rescale the two point sets such that the centroid of each set is 0 and the mean standard deviation \\(\\sqrt{2}\\) (equivalent to having the points distributed around a circled passing through the four corners of the \\([-1, 1]\\times[-1, 1]\\) square).</p> <p>This can done for every point as follows:</p> \\[ \\hat{p}^i = \\frac{\\sqrt{2}}{\\sigma}(p^i - \\mu) \\] <p>where \\(\\mu = (\\mu_x, \\mu_y) = \\frac{1}{N}\\sum_{i=1}^{n}p^i\\) is the centroid and \\(\\sigma = \\frac{1}{N}\\sum_{i=1}^{n}\\|p^i - \\mu\\|^2\\) is the mean standard deviation of the point set.</p> <p>This transformation can be expressed in matrix form using homogeneous coordinates:</p> \\[ \\hat{p}^i = \\begin{bmatrix} \\frac{\\sqrt{2}}{\\sigma} &amp; 0 &amp; -\\frac{\\sqrt{2}}{\\sigma}\\mu_x \\\\ 0 &amp; \\frac{\\sqrt{2}}{\\sigma} &amp; -\\frac{\\sqrt{2}}{\\sigma}\\mu_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <p>The normalized 8-point algorithm can therefore be summarized in three steps:</p> <ol> <li>Normalize the point correspondences: \\(\\hat{p}_1 = B_1 p_1\\), \\(\\hat{p}_2 = B_2 p_2\\)</li> <li>Estimate normalized \\(\\hat{F}\\) with 8-point algorithm using normalized coordinates \\(\\hat{p}_1\\), \\(\\hat{p}_2\\)</li> <li>Compute un-normalized \\(F\\) from \\(\\hat{F}\\).</li> </ol> \\[ \\hat{p}_2^T \\hat{F} \\hat{p}_1 = 0 \\qquad \\leftrightarrow \\qquad p_2^T b_2^T \\hat{F} B_1 p_1 = 0 \\] <p>Can \\(R\\), \\(T\\), \\(K_1\\) and \\(K_2\\) be extracted from \\(F\\)?</p> <ul> <li>In general, no: infinite solutions exist.</li> <li>However, if the coordinates of the principal points of each camera are known and the two cameras have the same focal length \\(f\\) in pixels, then \\(R\\), \\(T\\) and \\(f\\) can determined uniquely.</li> </ul>"},{"location":"Structure_from_Motion/3_uncalib_cams/#comparison-between-normalized-and-non-normalized-algorithm","title":"Comparison between Normalized and Non-Normalized Algorithm","text":"8-Point Normalized 8-Point Non-linear Refinement Avg. Ep. Line Distance 2.33 px 0.92 px 0.86 px"},{"location":"Structure_from_Motion/3_uncalib_cams/#error-measures","title":"Error Measures","text":"<p>The quality of the estimated Essential or Fundamental matrix can be measured using different error metrics:</p> <ul> <li>Algebraic error</li> <li>Directional Error</li> <li>Epipolar Line Distance</li> <li>Reprojection Error</li> </ul> <p> </p> <p>When in the error 0?</p> <ul> <li>These errors will be exactly 0 only if \\(E\\) (or \\(F\\)) is computed from just 8 points (because in this case a non-overdetermined solution exists).</li> <li>For more than 8 points, it will only be 0 if there is no noise or outliers in the data (if there is image noise or outliers then it the system becomes overdetermined)</li> </ul>"},{"location":"Structure_from_Motion/3_uncalib_cams/#algebraic-error","title":"Algebraic Error","text":"<p>This method follows directly from the 8-point algorithm, which seeks to minimize the algebraic error:</p> \\[ err = \\|QE\\|^2 = \\sum_{i=1}^N\\left( {\\overline{p}^i_2}^T E \\overline{p}^i_1 \\right)^2 \\] <p>From the proof of the epipolar constraint and using the definition of dot product, it can be observed that:</p> \\[ \\| \\overline{p}_2^T E \\overline{p}_1 \\| = \\| \\overline{p}_2^T \\cdot (E \\overline{p}_1) \\| = \\| \\overline{p}_2 \\| \\| E \\overline{p}_1 \\| \\cos(\\theta) = \\| \\overline{p}_2 \\| \\| [T_\\times] R \\overline{p}_1 \\| \\cos(\\theta) \\] <p>We can see that this product depends on the angle \\(\\theta\\) between \\(\\overline{p}_2\\) and the normal \\(n = E p_1\\) to epipolar plane. It is non-zero when \\(\\overline{p}_1\\), \\(\\overline{p}_2\\) and \\(T\\) are not coplanar.</p> <p>What is the drawback of this error measure?</p> <p>This method depends on the lengths of \\(\\overline{p}_1\\) and \\(\\overline{p}_2\\), we only care about the angle between \\(n\\) and \\(\\overline{p}_2\\), and this error metric changes based on the lengths of \\(\\overline{p}_1\\), \\(\\overline{p}_2\\) and the values of \\(E\\).</p>"},{"location":"Structure_from_Motion/3_uncalib_cams/#directional-error","title":"Directional Error","text":"<p>This error is defined as the sum of squared cosines of the angle from the epipolar plane:</p> \\[ err = \\sum_i \\left( \\cos(\\theta_i) \\right)^2 \\] <p>It is obtained by normalizing the algebraic error:</p> \\[ \\cos(\\theta) = \\frac{\\overline{p}_2^T E \\overline{p}_1}{\\|p_2\\| \\|E p_1\\|} \\]"},{"location":"Structure_from_Motion/3_uncalib_cams/#epipolar-line-distance","title":"Epipolar Line Distance","text":"<p>This method is defined as the sum of squared epipolar-line-to-point distances:</p> \\[ err = \\sum_{i=1}^N d^2 \\left(p_1^i, l_1^i \\right) + d^2 \\left(p_2^i, l_2^i \\right) \\] <p>This method is faster to compute than the reprojection error because it does not require point triangulation.</p> <p> </p>"},{"location":"Structure_from_Motion/3_uncalib_cams/#reprojection-error","title":"Reprojection Error","text":"<p>This method is defined as the sum of squared reprojection errors:</p> \\[ err = \\left\\| p_1 - \\pi(P, K_1, R, T) \\right\\|^2 + \\left\\| p_2 - \\pi(P, K_2, R, T) \\right\\|^2 \\] <p>This method is computationally more expensive than the previous three methods because it requires us to first triangulate the 3D points.</p> <p>However it is the most popular because of its accuracy. The reason is that the error is computed directly with respect to the raw input data, which are image points.</p> <p> </p> <ol> <li> <p>Richard I. Hartley, In Defense of the Eight-Point Algorithm \u21a9\u21a9</p> </li> </ol>"},{"location":"Structure_from_Motion/4_robust_sfm/","title":"Robust Structure from Motion","text":"<p>When matching points between two images or more, matched points are usually contaminated by outliers (i.e., wrong matches).</p> <p> </p> <p>Causes of outliers are:</p> <ul> <li>Repetitive features</li> <li>Changes in view point (including scale) and illumination</li> <li>Image noise</li> <li>Occlusions</li> <li>Image blur</li> </ul> <p>For reliable and accurate visual odometry, outliers must be removed. This is the task of robust estimation.</p>"},{"location":"Structure_from_Motion/4_robust_sfm/#effect-of-outliers-on-visual-odometry","title":"Effect of Outliers on Visual Odometry","text":"<p>In the example below, we can see how outliers can greatly impact the result of our estimation process, especially over large distances.</p> <p></p>"},{"location":"Structure_from_Motion/4_robust_sfm/#expectation-maximization-em-algorithm","title":"Expectation Maximization (EM) Algorithm","text":"<p>EM is a simple method for model fitting in the presence of outliers (very noisy points or wrong data).</p> <ul> <li>It can be applied to all sorts of problems where the goal is to estimate the parameters of a model from the data (e.g., camera calibration, Structure from Motion, DLT, PnP, P3P, Homography, etc.)</li> </ul> <p>Let\u2019s review EM applied to the line fitting problem.</p>"},{"location":"Structure_from_Motion/4_robust_sfm/#em-applied-to-line-fitting","title":"EM Applied to Line Fitting","text":"<ol> <li>Calculate model parameters that fit all data points      </li> <li>Estimate's Expectation: Calculate the residual error \\(r_i\\) for each data point and assign it a weight (e.g., \\(w_i=e^{-r_i^2}\\), where \\(r_i\\) is the point-to-line distance) representing the probability that such assignment is correct.      </li> <li>Maximization Step: Re-estimate line parameters (e.g., using weighted least-squares: \\(\\min\\sum w_i r_i^2\\)).      </li> <li>Iterate steps 2 and 3 until convergence.</li> <li>Select as inliers the data points with a weight higher than a given threshold.      </li> </ol>"},{"location":"Structure_from_Motion/4_robust_sfm/#problem-of-em-algorithm","title":"Problem of EM Algorithm","text":"<p>The EM algorithm is sensitive to the initial condition. It is not robust to outliers if the initial condition is far from the ground truth.</p> <p>Solutions: </p> <ul> <li>GNC Algorithm</li> <li>RANSAC Algorithm</li> </ul>"},{"location":"Structure_from_Motion/4_robust_sfm/#graduated-non-convexity-algorithm-gnc","title":"Graduated Non-Convexity algorithm (GNC)","text":"<p>At each iteration, EM estimates the model by minimizing the sum of squared residuals \\(\\sum w_i r_i^2\\). While this is a convex function, it is not robust to outliers.</p> <p>Idea of Graduated Non-Convexity (GNC)12:</p> <ul> <li>Optimize a surrogate function \\(\\sum \\rho_\\mu(r_i)\\), where \\(\\mu\\) controls the amount of non-convexity.</li> <li>Start by solving the non-robust convex optimization function (\\(\\mu\\rightarrow 0\\), i.e., least squares) and gradually increase non-convexity (\\(\\mu \\rightarrow \\infty\\)) until robustness is achieved.</li> <li>It is shown in 1 that GNC is robust up to 90% of outliers with fewer up to five times iterations than RANSAC.</li> </ul> <p> </p>"},{"location":"Structure_from_Motion/4_robust_sfm/#random-sample-consensus-ransac","title":"Random Sample Consensus (RANSAC)","text":"<p>RANSAC3 has become the standard method for model fitting in the presence of outliers (very noisy points or wrong data).</p> <ul> <li>It is non-deterministic: you get a different result every time you run it.</li> <li>Significantly outperforms the EM algorithm, it is not sensitive to the initial condition, and does not get stuck in local maxima.</li> <li>It can be applied to all sorts of problems where the goal is to estimate the parameters of a model from the data (e.g., camera calibration, Structure from Motion, DLT, PnP, P3P, Homography, etc.)</li> </ul> <p>Let\u2019s review RANSAC for line fitting and see how we can use it to do Structure from Motion</p> <ol> <li>Select sample of 2 points at random from a set of points.</li> <li>Calculate model parameters that fit the data in the sample.</li> <li>Calculate the residual error for each data point.</li> <li>Select data that support current hypothesis.</li> <li>Repeat steps 1-4 \\(k\\) times.</li> <li>Select the set with the maximum number of inliers obtained within \\(k\\) iterations.</li> </ol> <p>How many iterations does RANSAC need?</p> <p>Ideally: Check all possible combinations of 2 points in a dataset of \\(N\\) points.</p> <p>Number of pairwise combinations: \\(\\frac{N(N-1)}{2}\\)</p> <p>This value can be computationally unfeasible if \\(N\\) is too large. Example: For 1000 points you need to check all \\(1000\\cdot 999/2\\simeq 500000\\) possibilities</p> <p>Do we really need to check all possibilities or can we stop RANSAC after some iterations?</p> <p>Checking a subset of combinations is enough if we have a rough estimate of the percentage of inliers in our dataset.</p> <p>This can be done in a probabilistic way.</p>"},{"location":"Structure_from_Motion/4_robust_sfm/#computing-number-of-ransac-iterations","title":"Computing Number of RANSAC Iterations","text":"<p>For this section, let us use the following notation:</p> <ul> <li>\\(N\\): Total Number of data points</li> <li>\\(w = \\frac{\\text{number of inliers}}{N}\\)</li> <li>\\(W = P\\)(selecting an inlier-point out of the dataset)</li> </ul> <p>Assumption</p> <p>The 2 points necessary to estimation a line are selected independently</p> <ul> <li>\\(w^2 = P\\)(both selected points are inliers)</li> <li>\\(1-w^2 = P\\)(at least one of these points is an outlier)</li> </ul> <p>Let \\(k\\) be the number of RANSAC iterations executed so far.</p> <ul> <li>\\((1-w^2)^k = P\\)(RANSAC never selected two points that are both inliers)</li> </ul> <p>Let \\(p = P\\)(probability of success)</p> <ul> <li>\\(1-p = (1-w^2)^k\\)</li> </ul> <p>Therefore the number of required iterations \\(k\\) can be calculated as follows:</p> \\[ k = \\frac{\\log(1-p)}{\\log(1-w^2)} \\] <p>By knowing the fraction of inliers \\(w\\), after \\(k\\) RANSAC iterations we will have a probability \\(p\\) of finding a set of points free of outliers.</p> <p>Example</p> <p>If we want a probability of success \\(p=99\\%\\) and we know that \\(w = 50\\%\\), then \\(k=16\\) iterations.</p> <p>From the example above, we can see that this is far fewer than trying out all possible combinations.</p> <p>Important</p> <p>The number of points does not influence the minimum number of iterations (\\(k\\)), only \\(w\\) does!</p>"},{"location":"Structure_from_Motion/4_robust_sfm/#ransac-applied-to-general-model-fitting","title":"RANSAC applied to General Model Fitting","text":"<ol> <li>Initial - Let \\(A\\) be a set of \\(N\\) points</li> <li> <p>Repeat until maximum number of iterations \\(k\\) reached:</p> <ol> <li>Randomly select a sample of \\(s\\) points from \\(A\\)</li> <li>Fit a model from the \\(s\\) points</li> <li>Compute the distances of all other points from this model</li> <li>Construct the inlier set (i.e. count the number of points whose distance \\(&lt;d\\))</li> <li>Store these inliers</li> </ol> </li> <li> <p>The set with the maximum number of inliers is chosen as a solution to the problem</p> </li> </ol> <p>Tip</p> <p>The formula for calculating the number of iterations is commonly written as a function of the fraction of outliers \\(\\epsilon\\)</p> \\[ k = \\frac{\\log(1-p)}{\\log(1-(1-\\epsilon)^s)} \\]"},{"location":"Structure_from_Motion/4_robust_sfm/#the-three-key-ingredients-of-ransac","title":"The Three Key Ingredients of RANSAC","text":"<p>In order to implement RANSAC for Structure From Motion (SFM), we need three key ingredients:</p> <ol> <li> <p>What\u2019s the model in SFM?</p> <ul> <li>The essential matrix (for calibrated cameras) or the fundamental matrix (for uncalibrated cameras)</li> <li>Alternatively, \\(R\\) and \\(T\\)</li> </ul> </li> <li> <p>What\u2019s the minimum number of points to estimate the model?</p> <ul> <li>We know that 5 points is the theoretical minimum number of points for calibrated cameras</li> <li>However, if we use the 8-point algorithm, then 8 is the minimum (for both calibrated or uncalibrated cameras)</li> </ul> </li> <li> <p>How do we compute the distance of a point from the model?</p> <ul> <li>Algebraic error</li> <li>Directional error</li> <li>Epipolar line distance</li> <li>Reprojection error</li> </ul> </li> </ol>"},{"location":"Structure_from_Motion/4_robust_sfm/#applying-8-point-ransac-to-sfm-problem","title":"Applying 8-Point RANSAC to SfM Problem","text":"<p>Let\u2019s consider the following image pair and its image correspondences (e.g., Harris, SIFT, etc.), denoted by arrows:</p> <p> </p> <p>For convenience, we overlay the features of the second image on the first image and use arrows to denote the motion vectors of the features:</p> <p></p> <ol> <li> <p>Randomly select 8 point correspondences:     </p> </li> <li> <p>Fit the model to all other points and count the inliers:     </p> </li> <li> <p>Repeat steps 1-2 \\(k\\) times, where \\(k\\) is:</p> \\[ k = \\frac{\\log(1-p)}{\\log(1-(1-\\epsilon)^8)} \\] </li> </ol>"},{"location":"Structure_from_Motion/4_robust_sfm/#ransac-iterations-k-vs-s","title":"RANSAC Iterations \\(k\\) Vs. \\(s\\)","text":"<p>\\(k\\) is exponential in the number of points \\(s\\) necessary to estimate the model:</p> <ul> <li> <p>8-point RANSAC</p> <ul> <li>Assuming:<ul> <li>\\(p = 99\\%\\)</li> <li>\\(\\epsilon = 50\\%\\) (fraction of outliers)</li> <li>\\(s = 8\\) points (8-point algorithm)</li> </ul> </li> </ul> <p>\\(k = 1177\\) iterations</p> </li> <li> <p>5-point RANSAC</p> <ul> <li>Assuming:<ul> <li>\\(p = 99\\%\\)</li> <li>\\(\\epsilon = 50\\%\\) (fraction of outliers)</li> <li>\\(s = 5\\) points (5-point algorithm)</li> </ul> </li> </ul> <p>\\(k = 145\\) iterations</p> </li> <li> <p>2-point RANSAC (e.g., line fitting)</p> <ul> <li>Assuming:<ul> <li>\\(p = 99\\%\\)</li> <li>\\(\\epsilon = 50\\%\\) (fraction of outliers)</li> <li>\\(s = 2\\) points</li> </ul> </li> </ul> <p>\\(k = 16\\) iterations</p> </li> </ul> <p>As observed, \\(k\\) is exponential in the number of points \\(s\\) necessary to estimate the model.</p> <ul> <li>The 8-point algorithm is extremely simple and was very successful; however, it requires more than 1177 iterations</li> <li>Because of this, there has been a large interest by the research community in using smaller motion parameterizations (i.e., smaller \\(s\\))</li> <li>The first efficient solution to the minimal-case solution (5-point algorithm) took almost a century (Kruppa 1913 \u2192 Nister 2004)</li> <li>The 5-point RANSAC (Nister 2004) only requires 145 iterations; however:<ul> <li>The 5-point algorithm can return up to 10 solutions of \\(E\\) (worst case scenario)</li> <li>The 8-point algorithm only returns a unique solution of \\(E\\)</li> </ul> </li> </ul> <p>Is it possible to use less than 5 points for RANSAC?</p> <p>Yes, if you use motion constraints!</p>"},{"location":"Structure_from_Motion/4_robust_sfm/#planar-motion-constraint","title":"Planar Motion Constraint","text":"<p>Planar motion is described by three parameters: \\(\\theta\\), \\(\\phi\\), \\(\\rho\\):</p> \\[ R = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\qquad T = \\begin{bmatrix} \\rho\\cos\\phi \\\\ \\rho\\sin\\phi \\\\ 0 \\end{bmatrix} \\] <p>Let\u2019s compute the epipolar geometry:</p> <ul> <li>Essential matrix: \\(E = [T_\\times]R\\)</li> <li>Epipolar constraint: \\(\\overline{p}_2^T E \\overline{p}_1 = 0\\)</li> </ul> <p>Observe that \\(E\\) has 2 DoF (\\(\\theta\\), \\(\\phi\\), because \\(\\rho\\) is the scale factor); thus, 2 correspondences are sufficient to estimate \\(\\theta\\) and \\(\\phi\\)4.</p> \\[ E = [T_\\times]R = \\begin{bmatrix} 0 &amp; 0 &amp; \\rho\\sin\\phi \\\\ 0 &amp; 0 &amp; -\\rho\\cos\\phi \\\\ -\\rho\\sin(\\phi-\\theta) &amp; \\rho\\cos(\\phi-\\theta) &amp; 0 \\end{bmatrix} \\] <p>Can we use less than 2 point correspondences?</p> <p>Yes, if we exploit wheeled vehicles with non-holonomic constraints.</p> <p>Wheeled vehicles, like cars, follow locally-planar circular motion about the Instantaneous Center of Rotation (ICR).</p> <p>Therefore \\(\\phi = \\theta/2\\) \u2192 giving only 1 DoF (\\(\\theta\\)). Thus, only 1 point correspondence is sufficient5.</p> <p>In this case, the essential matrix is therefore:</p> \\[ E = [T_\\times]R = \\begin{bmatrix} 0 &amp; 0 &amp; \\rho\\sin\\frac{\\theta}{2} \\\\ 0 &amp; 0 &amp; \\rho\\cos\\frac{\\theta}{2} \\\\ \\rho\\sin\\frac{\\theta}{2} &amp; -\\rho\\cos\\frac{\\theta}{2} &amp; 0 \\end{bmatrix} \\] <p>and the epipolar constraint gives us the following equation:</p> \\[ \\theta = -2\\tan^{-1}\\left(\\frac{v_2 - v_1}{u_2 + u_1} \\right) \\] <p>As only one degree of freedom needs to be estimated, we only need 1 iteration to find the inliers.</p>"},{"location":"Structure_from_Motion/4_robust_sfm/#state-of-the-art","title":"State of the Art","text":""},{"location":"Structure_from_Motion/4_robust_sfm/#differentiable-ransac","title":"Differentiable RANSAC","text":"<p>Random Sample Consensus (RANSAC)6 is not differentiable since it relies on selecting a hypothesis based on maximizing the number of inliers (i.e., \\(\\arg\\max\\)).</p> <ul> <li>DSAC shows how sample consensus can be used in a differentiable way</li> <li>This enables the use of sample consensus in a variety of learning tasks.</li> </ul>"},{"location":"Structure_from_Motion/4_robust_sfm/#deep-fundamental-matrix-estimation","title":"Deep Fundamental Matrix Estimation","text":"<p>Deep Fundamental Matrix Estimation7 takes in two sets of noisy local features (coordinates + descriptors) contaminated by outliers and outputs the fundamental matrix.</p> <p>Idea: Solve a weighted homogeneous least-squares problem, where robust weights are estimated using deep networks. Robust: Handles extreme wide-baseline image pairs</p>"},{"location":"Structure_from_Motion/4_robust_sfm/#superglue","title":"SuperGlue","text":"<p>SuperGlue8 is a learning feature matching with graph neural networks.</p> <ul> <li>Input: Two sets of noisy local features (coordinates + descriptors) contaminated by outliers.</li> <li>Output: Strong &amp; outlier-free matches</li> <li>Combines deep learning with classical optimization (Graph Neural Networks, Attention, Optimal Transport)</li> <li>Robust: Handles extreme wide-baseline image pairs</li> </ul> <ol> <li> <p>Yang, Antonante, Tzoumas, Carlone, Graduated Non-Convexity for Robust Spatial Perception: From Non-Minimal Solvers to Global Outlier Rejection - International Conference on Robotics and Automation (ICRA), 2020 - PDF \u21a9\u21a9</p> </li> <li> <p>Blake, Zisserman, Visual Reconstruction - MIT Press, Cambridge, Massachusetts, 1987\u00a0\u21a9</p> </li> <li> <p>M. A.Fischler and R. C.Bolles. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography - Graphics and Image Processing, 1981 - PDF \u21a9</p> </li> <li> <p>\"2-Point RANSAC\", Ortin &amp; Montiel, Indoor Robot Motion Based on Monocular Images - Robotica, 2001 - PDF \u21a9</p> </li> <li> <p>Scaramuzza, 1-Point-RANSAC Structure from Motion for Vehicle-Mounted Cameras by Exploiting Non-Holonomic Constraints - International Journal of Computer Vision, 2011 - PDF \u21a9</p> </li> <li> <p>E. Brachmann et al., DSAC - Differentiable RANSAC for Camera Localization - International Conference on Computer Vision and Pattern Recognition (CVPR), 2017 - PDF, Video \u21a9</p> </li> <li> <p>Ranftl, Koltun, Deep Fundamental Matrix Estimation - European Conference on Computer Vision (ECCV), 2018 - PDF \u21a9</p> </li> <li> <p>Sarlin, DeTone, Malisiewicz, Rabinovich, SuperGlue: Learning Feature Matching with Graph Neural Networks - International Conference on Computer Vision and Pattern Recognition (CVPR), 2020 - PDF, Code \u21a9</p> </li> </ol>"},{"location":"Visual_Odometry/1_vo_overview/","title":"Visual Odometry Overview","text":""},{"location":"Visual_Odometry/1_vo_overview/#what-is-visual-odometry-vo","title":"What is Visual Odometry (VO)?","text":"<p>VO is the process of incrementally estimating the pose of the vehicle by examining the changes that motion induces on the images of its onboard cameras.</p> <p> </p>"},{"location":"Visual_Odometry/1_vo_overview/#why-vo","title":"Why VO?","text":"<ul> <li>VO is crucial for flying, walking, and underwater robots</li> <li>Contrary to wheel odometry, VO is not affected by wheel slippage (e.g., on sand or wet floor) (wheel odometry error can be up to 10%)</li> <li>Very accurate: relative position error is 0.1% \u2212 2% of the travelled distance</li> <li>VO can be used as a complement to<ul> <li>Wheel encoders (wheel odometry)</li> <li>GPS (when GPS is degraded)</li> <li>Inertial Measurement Units (IMUs)</li> <li>Laser odometry</li> </ul> </li> </ul>"},{"location":"Visual_Odometry/1_vo_overview/#assumptions","title":"Assumptions","text":"<ul> <li>Sufficient illumination in the environment</li> <li>Dominance of static scene over moving objects</li> <li>Enough texture to allow apparent motion to be extracted</li> <li>Sufficient scene overlap between consecutive frames</li> </ul>"},{"location":"Visual_Odometry/1_vo_overview/#brief-history-of-vo","title":"Brief History of VO","text":"<ul> <li>1980: First known VO real-time implementation on a robot by Hans Moraveck PhD thesis (NASA/JPL) for Mars rovers using one sliding camera (sliding stereo).</li> <li>1980 to 2000: The VO research was dominated by NASA/JPL in preparation of the 2004 mission to Mars</li> <li>2004: VO was used on a robot on another planet: Mars rovers Spirit and Opportunity (see seminal paper from NASA/JPL, 2007)</li> <li>2004: VO was revived in the academic environment by David Nister\u2019s \u00abVisual Odometry\u00bb paper. The term VO became popular</li> <li>2015-today: VO becomes a fundamental tool of several products: VR/AR, drones, smartphones</li> <li>2021: VO is used on the Mars helicopter Ingenuity</li> </ul>"},{"location":"Visual_Odometry/1_vo_overview/#vo-vs-vslam-vs-sfm","title":"VO Vs. VSLAM Vs. SFM","text":""},{"location":"Visual_Odometry/1_vo_overview/#vo-vs-structure-from-motion-sfm","title":"VO Vs. Structure from Motion (SFM)","text":"<p>SFM is more general than VO and tackles the problem of 3D reconstruction and 6DOF pose estimation from unordered image sets.</p> <ul> <li>VO is a particular case of SFM.</li> <li>VO focuses on estimating the 6DoF motion of the camera sequentially (as a new frame arrives) and in real time.</li> <li>Terminology: sometimes SFM is used as a synonym of VO</li> </ul> <p>VO Vs. Visual SLAM (VSLAM)</p> <ul> <li>Visual Odometry<ul> <li>Focus on incremental estimation</li> <li>Guarantees local consistency (i.e., estimated trajectory is locally correct, but not globally, i.e. from the start to the end)</li> </ul> </li> <li>Visual SLAM (Simultaneous Localization And Mapping)<ul> <li>SLAM = visual odometry + loop detection &amp; closure</li> <li>Guarantees global consistency (the estimated trajectory is globally correct, i.e. from the start to the end)</li> </ul> </li> </ul> <p> </p>"},{"location":"Visual_Odometry/1_vo_overview/#vo-flow-chart","title":"VO Flow Chart","text":"<p>VO computes the camera path incrementally (pose after pose).</p> <pre><code>stateDiagram-v2\n    s1 : Image Sequence\n    s2 : Feature Detection\n    s3 : Feature Matching (Tracking)\n    s4 : Motion Estimation\n    s5 : Local Optimization\n\n    s1 --&gt; s2\n    s2 --&gt; s3\n    s3 --&gt; s4\n    s4 --&gt; s5</code></pre> <p>The steps involving feature detection to motion estimation are known as the \"front-end\". They output the relative post between the last two frames.</p> <p>The last step (local optimization) adjusts the relative poses amongst multiple recent frames.</p> <p>VO studies the incremental camera path computation (frame after frame).</p>"},{"location":"Visual_Odometry/2_image_formation/","title":"Image Formation","text":"Tldr"},{"location":"Visual_Odometry/2_image_formation/#historical-context","title":"Historical Context","text":"<ul> <li>Pinhole Model: Mozi (470-390 BCE), Aristotle (384-322 BCE)</li> <li>Principles of optics (including lenses): Alhacen (965-1039)</li> <li>Camera obscura: Leonardo da Vinci (1452-1519), Johann Zahn (1631-1707)</li> <li>First Photo (Heliography): Joseph Nicephore Niepce (1822)</li> <li>Daguerr\u00e9otypes (1839)</li> <li>Photographic film (Eastman, 1888, founder of Kodak)</li> <li>Cinema (Lumi\u00e8re Brothers, 1895)</li> <li>Color Photography (Lumi\u00e8re Brothers, 1908)</li> <li>Television (Baird, Farnsworth, Zworykin, 1920s)</li> <li>First consumer camera with CCD: Sony Mavica (1981)</li> <li>First fully digital camera: Kodak DCS100 (1990)</li> </ul>"},{"location":"Visual_Odometry/2_image_formation/#image-formation-how-are-objects-in-the-world-captured-in-an-image","title":"Image Formation - How are objects in the world captured in an image?","text":"<p>Would such a setup work?  </p> <p>The problem here is that all light rays from every point in the world are being seen by the film, making the image white.</p> <p>A first solution is to add a barrier to block off most rays.</p> <ul> <li>This reduces blurring</li> <li>The measurement of the opening is known as the aperture.</li> </ul> <p> </p> <p>This solution is commonly referred to as the \"Camera Obscura\".</p> <ul> <li>In Latin, camera obscura means \"dark room\"</li> <li>Basic principle known to Chinese philosopher Mozi (470-390 BC) and Aristotle (384-322 BC)</li> <li>Drawing aid for artists: described by Leonardo da Vinci (1452-1519)</li> </ul>"},{"location":"Visual_Odometry/2_image_formation/#effects-of-aperture-size","title":"Effects of Aperture Size","text":"<p>A large aperture makes the image blurry because multiple rays of light are let through from each world\u2019s point.  </p> <p>By shrinking the aperture, the image becomes sharper. The ideal aperture is a pinhole that only lets through one ray of light from each world's point.</p> <p> </p>"},{"location":"Visual_Odometry/2_image_formation/#why-not-make-the-aperture-as-small-as-possible","title":"Why not make the aperture as small as possible?","text":"<ul> <li>With small apertures, less light gets through \u2192 must increase exposure time.</li> <li>The aperture gets too small, diffraction effects start to appear.</li> <li>The optimal size of aperture depends on the focal length as well.</li> </ul>"}]}